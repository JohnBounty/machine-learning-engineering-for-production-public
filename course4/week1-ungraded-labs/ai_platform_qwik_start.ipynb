{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Platform: Qwik Start\n",
    "\n",
    "This lab gives you an introductory, end-to-end experience of training and prediction on AI Platform. The lab will use a census dataset to:\n",
    "\n",
    "- Create a TensorFlow 2.x training application and validate it locally.\n",
    "- Run your training job on a single worker instance in the cloud.\n",
    "- Deploy a model to support prediction.\n",
    "- Request an online prediction and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get your training data\n",
    "\n",
    "The relevant data files, adult.data and adult.test, are hosted in a public Cloud Storage bucket.\n",
    "\n",
    "You can read the files directly from Cloud Storage or copy them to your local environment. For this lab you will download the samples for local training, and later upload them to your own Cloud Storage bucket for cloud training.\n",
    "\n",
    "Run the following command to download the data to a local file directory and set variables that point to the downloaded data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/ml-engine/census/data/adult.data.csv...\n",
      "Copying gs://cloud-samples-data/ml-engine/census/data/adult.test.csv...\n",
      "Copying gs://cloud-samples-data/ml-engine/census/data/census.train.csv...       \n",
      "Copying gs://cloud-samples-data/ml-engine/census/data/census.test.csv...        \n",
      "Copying gs://cloud-samples-data/ml-engine/census/data/test.csv...\n",
      "Copying gs://cloud-samples-data/ml-engine/census/data/test.json...              \n",
      "/ [6/6 files][ 10.7 MiB/ 10.7 MiB] 100% Done                                    \n",
      "Operation completed over 6 objects/10.7 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir data\n",
    "gsutil -m cp gs://cloud-samples-data/ml-engine/census/data/* data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export TRAIN_DATA=$(pwd)/data/adult.data.csv\n",
    "export EVAL_DATA=$(pwd)/data/adult.test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect what the data looks like by looking at the first couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n",
      "50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n",
      "38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n",
      "53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n",
      "28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n",
      "37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n",
      "49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n",
      "52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n",
      "31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n",
      "42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head data/adult.data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run a local training job\n",
    "\n",
    "A local training job loads your Python training program and starts a training process in an environment that's similar to that of a live Cloud AI Platform cloud training job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Create files to hold the Python program\n",
    "\n",
    "To do that, let's create three files. The first, called util.py, will contain utility methods for cleaning and preprocessing the data, as well as performing any feature engineering needed by transforming and normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/util.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Storage directory\n",
    "DATA_DIR = os.path.join(tempfile.gettempdir(), 'census_data')\n",
    "\n",
    "# Download options.\n",
    "DATA_URL = (\n",
    "    'https://storage.googleapis.com/cloud-samples-data/ai-platform/census'\n",
    "    '/data')\n",
    "TRAINING_FILE = 'adult.data.csv'\n",
    "EVAL_FILE = 'adult.test.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "# These are the features in the dataset.\n",
    "# Dataset information: https://archive.ics.uci.edu/ml/datasets/census+income\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "# This is the label (target) we want to predict.\n",
    "_LABEL_COLUMN = 'income_bracket'\n",
    "\n",
    "# These are columns we will not use as features for training. There are many\n",
    "# reasons not to use certain attributes of data for training. Perhaps their\n",
    "# values are noisy or inconsistent, or perhaps they encode bias that we do not\n",
    "# want our model to learn. For a deep dive into the features of this Census\n",
    "# dataset and the challenges they pose, see the Introduction to ML Fairness\n",
    "# Notebook: https://colab.research.google.com/github/google/eng-edu/blob\n",
    "# /master/ml/cc/exercises/intro_to_fairness.ipynb\n",
    "UNUSED_COLUMNS = ['fnlwgt', 'education', 'gender']\n",
    "\n",
    "_CATEGORICAL_TYPES = {\n",
    "    'workclass': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
    "        'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
    "    ]),\n",
    "    'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
    "        'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'\n",
    "    ]),\n",
    "    'occupation': pd.api.types.CategoricalDtype([\n",
    "        'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
    "        'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
    "        'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
    "        'Sales', 'Tech-support', 'Transport-moving'\n",
    "    ]),\n",
    "    'relationship': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
    "        'Wife'\n",
    "    ]),\n",
    "    'race': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'\n",
    "    ]),\n",
    "    'native_country': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
    "        'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
    "        'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong',\n",
    "        'Hungary',\n",
    "        'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos',\n",
    "        'Mexico',\n",
    "        'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines',\n",
    "        'Poland',\n",
    "        'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand',\n",
    "        'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
    "    ]),\n",
    "    'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
    "        '<=50K', '>50K'\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "def _download_and_clean_file(filename, url):\n",
    "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\n",
    "\n",
    "    The CSVs may use spaces after the comma delimters (non-standard) or include\n",
    "    rows which do not represent well-formed examples. This function strips out\n",
    "    some of these problems.\n",
    "\n",
    "    Args:\n",
    "      filename: filename to save url to\n",
    "      url: URL of resource to download\n",
    "    \"\"\"\n",
    "    temp_file, _ = urllib.request.urlretrieve(url)\n",
    "    with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
    "        with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
    "            for line in temp_file_object:\n",
    "                line = line.strip()\n",
    "                line = line.replace(', ', ',')\n",
    "                if not line or ',' not in line:\n",
    "                    continue\n",
    "                if line[-1] == '.':\n",
    "                    line = line[:-1]\n",
    "                line += '\\n'\n",
    "                file_object.write(line)\n",
    "    tf.io.gfile.remove(temp_file)\n",
    "\n",
    "\n",
    "def download(data_dir):\n",
    "    \"\"\"Downloads census data if it is not already present.\n",
    "\n",
    "    Args:\n",
    "      data_dir: directory where we will access/save the census data\n",
    "    \"\"\"\n",
    "    tf.io.gfile.makedirs(data_dir)\n",
    "\n",
    "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
    "    if not tf.io.gfile.exists(training_file_path):\n",
    "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
    "\n",
    "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
    "    if not tf.io.gfile.exists(eval_file_path):\n",
    "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
    "\n",
    "    return training_file_path, eval_file_path\n",
    "\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      dataframe: Pandas dataframe with raw data\n",
    "\n",
    "    Returns:\n",
    "      Dataframe with preprocessed data\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
    "    dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = dataframe.select_dtypes(['object']).columns\n",
    "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(\n",
    "        _CATEGORICAL_TYPES[x.name]))\n",
    "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def standardize(dataframe):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      dataframe: Pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "      Input dataframe with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == 'float32':\n",
    "            dataframe[column] -= dataframe[column].mean()\n",
    "            dataframe[column] /= dataframe[column].std()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y)\n",
    "    dataframes.\n",
    "\n",
    "    Returns:\n",
    "      A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
    "      Pandas dataframes with features for training and train_y and eval_y are\n",
    "      numpy arrays with the corresponding labels.\n",
    "    \"\"\"\n",
    "    # Download Census dataset: Training and eval csv files.\n",
    "    training_file_path, eval_file_path = download(DATA_DIR)\n",
    "\n",
    "    # This census data uses the value '?' for missing entries. We use\n",
    "    # na_values to\n",
    "    # find ? and set it to NaN.\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv\n",
    "    # .html\n",
    "    train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS,\n",
    "                           na_values='?')\n",
    "    eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values='?')\n",
    "\n",
    "    train_df = preprocess(train_df)\n",
    "    eval_df = preprocess(eval_df)\n",
    "\n",
    "    # Split train and eval data with labels. The pop method copies and removes\n",
    "    # the label column from the dataframe.\n",
    "    train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
    "    eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
    "\n",
    "    # Join train_x and eval_x to normalize on overall means and standard\n",
    "    # deviations. Then separate them again.\n",
    "    all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
    "    all_x = standardize(all_x)\n",
    "    train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
    "\n",
    "    # Reshape label columns for use with tf.data.Dataset\n",
    "    train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
    "    eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))\n",
    "\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file, called model.py, defines the input function and the model architecture. In this example, we use tf.data API for the data pipeline and create the model using the Keras Sequential API. We define a DNN with an input layer and 3 additonal layers using the Relu activation function. Since the task is a binary classification, the output layer uses the sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "    \"\"\"Generates an input function to be used for model training.\n",
    "\n",
    "    Args:\n",
    "      features: numpy array of features used for training or inference\n",
    "      labels: numpy array of labels for each example\n",
    "      shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "        training, False for evaluation)\n",
    "      num_epochs: number of epochs to provide the data for\n",
    "      batch_size: batch size for training\n",
    "\n",
    "    Returns:\n",
    "      A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "        evaluation\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_keras_model(input_dim, learning_rate):\n",
    "    \"\"\"Creates Keras Model for Binary Classification.\n",
    "\n",
    "    The single output node + Sigmoid activation makes this a Logistic\n",
    "    Regression.\n",
    "\n",
    "    Args:\n",
    "      input_dim: How many features the input has\n",
    "      learning_rate: Learning rate for training\n",
    "\n",
    "    Returns:\n",
    "      The compiled Keras model (still needs to be trained)\n",
    "    \"\"\"\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(100, activation=tf.nn.relu, kernel_initializer='uniform',\n",
    "                  input_shape=(input_dim,)),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),\n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    # Custom Optimizer:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last file, called task.py, trains on data loaded and preprocessed in util.py. Using the tf.distribute.MirroredStrategy() scope, it is possible to train on a distributed fashion. The trained model is then saved in a TensorFlow SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='local or GCS location for writing checkpoints and exporting '\n",
    "             'models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=128')\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        default=.01,\n",
    "        type=float,\n",
    "        help='learning rate for gradient descent, default=.01')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "      args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "\n",
    "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
    "\n",
    "    # dimensions\n",
    "    num_train_examples, input_dim = train_x.shape\n",
    "    num_eval_examples = eval_x.shape[0]\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(\n",
    "        input_dim=input_dim, learning_rate=args.learning_rate)\n",
    "\n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    training_dataset = model.input_fn(\n",
    "        features=train_x.values,\n",
    "        labels=train_y,\n",
    "        shuffle=True,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    validation_dataset = model.input_fn(\n",
    "        features=eval_x.values,\n",
    "        labels=eval_y,\n",
    "        shuffle=False,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=num_eval_examples)\n",
    "\n",
    "    # Setup Learning Rate decay.\n",
    "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
    "        verbose=True)\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
    "        histogram_freq=1)\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        training_dataset,\n",
    "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
    "        epochs=args.num_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
    "\n",
    "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
    "    tf.keras.models.save_model(keras_model, export_path)\n",
    "    print('Model exported to: {}'.format(export_path))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        args = get_args()\n",
    "        tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
    "        train_and_evaluate(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Run a training job locally using the Python training program\n",
    "\n",
    "**NOTE** When you run the same training job on AI Platform later in the lab, you'll see that the command is not much different from the above.\n",
    "\n",
    "Specify an output directory and set a MODEL_DIR variable to hold the trained model, then run the training job locally by running the following command (by default, verbose logging is turned off. You can enable it by setting the --verbosity tag to DEBUG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 16:30:20.517938: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-28 16:30:20.517988: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-28 16:30:20.518027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (census-income-lab): /proc/driver/nvidia/version does not exist\n",
      "2023-09-28 16:30:20.523584: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2023-09-28 16:30:22.393858: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-09-28 16:30:22.393921: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-09-28 16:30:22.398716: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2023-09-28 16:30:22.461363: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 11\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-09-28 16:30:22.686138: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2023-09-28 16:30:22.692578: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 16:30:25.649796: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-09-28 16:30:25.650971: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-09-28 16:30:25.658972: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-09-28 16:30:25.668899: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2023-09-28 16:30:25.684855: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25\n",
      "\n",
      "2023-09-28 16:30:25.691036: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.trace.json.gz\n",
      "2023-09-28 16:30:25.708803: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25\n",
      "\n",
      "2023-09-28 16:30:25.711494: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.memory_profile.json.gz\n",
      "2023-09-28 16:30:25.713381: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25\n",
      "Dumped tool data for xplane.pb to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.xplane.pb\n",
      "Dumped tool data for overview_page.pb to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to output/keras_tensorboard/train/plugins/profile/2023_09_28_16_30_25/census-income-lab.kernel_stats.pb\n",
      "\n",
      "2023-09-28 16:30:26.397488: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 11\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 5s 9ms/step - loss: 0.5004 - accuracy: 0.7932 - val_loss: 0.3612 - val_accuracy: 0.8308\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.015.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3608 - accuracy: 0.8340 - val_loss: 0.3325 - val_accuracy: 0.8399\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3444 - accuracy: 0.8410 - val_loss: 0.3373 - val_accuracy: 0.8475\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.01125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3382 - accuracy: 0.8443 - val_loss: 0.3327 - val_accuracy: 0.8474\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.010625.\n",
      "254/254 [==============================] - 1s 4ms/step - loss: 0.3339 - accuracy: 0.8457 - val_loss: 0.3329 - val_accuracy: 0.8483\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0103125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3309 - accuracy: 0.8464 - val_loss: 0.3415 - val_accuracy: 0.8456\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.01015625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3308 - accuracy: 0.8464 - val_loss: 0.3217 - val_accuracy: 0.8492\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.010078125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3278 - accuracy: 0.8477 - val_loss: 0.3272 - val_accuracy: 0.8476\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0100390625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3280 - accuracy: 0.8484 - val_loss: 0.3226 - val_accuracy: 0.8468\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.01001953125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.8488 - val_loss: 0.3289 - val_accuracy: 0.8489\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.010009765625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3249 - accuracy: 0.8493 - val_loss: 0.3489 - val_accuracy: 0.8460\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.010004882812500001.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.8483 - val_loss: 0.3259 - val_accuracy: 0.8486\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.01000244140625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3258 - accuracy: 0.8481 - val_loss: 0.3355 - val_accuracy: 0.8378\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.010001220703125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3241 - accuracy: 0.8490 - val_loss: 0.3245 - val_accuracy: 0.8484\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0100006103515625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3259 - accuracy: 0.8484 - val_loss: 0.3221 - val_accuracy: 0.8519\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.01000030517578125.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3259 - accuracy: 0.8486 - val_loss: 0.3258 - val_accuracy: 0.8488\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.010000152587890625.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3233 - accuracy: 0.8498 - val_loss: 0.3360 - val_accuracy: 0.8420\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.010000076293945313.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3260 - accuracy: 0.8494 - val_loss: 0.3254 - val_accuracy: 0.8532\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.010000038146972657.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3259 - accuracy: 0.8497 - val_loss: 0.3348 - val_accuracy: 0.8513\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.010000019073486329.\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.3265 - accuracy: 0.8490 - val_loss: 0.3334 - val_accuracy: 0.8378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 16:30:46.236790: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: output/keras_export/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to: output/keras_export\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Pool.__del__ at 0x7fc9951f2550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/pool.py\", line 268, in __del__\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/queues.py\", line 371, in put\n",
      "AttributeError: 'NoneType' object has no attribute 'dumps'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_DIR=output\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --package-path trainer/ \\\n",
    "    --job-dir $MODEL_DIR \\\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output has been written to the output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\n",
      "keras_metadata.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls output/keras_export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Prepare input for prediction\n",
    "\n",
    "To receive valid and useful predictions, you must preprocess input for prediction in the same way that training data was preprocessed. In a production system, you may want to create a preprocessing pipeline that can be used identically at training time and prediction time.\n",
    "\n",
    "For this exercise, use the training package's data-loading code to select a random sample from the evaluation data. This data is in the form that was used to evaluate accuracy after each epoch of training, so it can be used to send test predictions without further preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following snippet of code to preprocess the raw data from the adult.test.csv file. Here, we are grabbing 5 examples to run predictions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import util\n",
    "_, _, eval_x, eval_y = util.load_data()\n",
    "\n",
    "prediction_input = eval_x.sample(5)\n",
    "prediction_targets = eval_y[prediction_input.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the numerical representation of the features by printing the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            age  workclass  education_num  marital_status  occupation  \\\n",
      "7061   0.901213          4      -0.030304               2           3   \n",
      "9169   1.630560          3      -0.808226               2           2   \n",
      "6388  -1.432697          6      -0.030304               4           0   \n",
      "15117 -0.265742          3      -0.419265               2           0   \n",
      "5239   1.557625          3       1.136580               2           9   \n",
      "\n",
      "       relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
      "7061              0     4     -0.144792      4.688376        1.418577   \n",
      "9169              5     2     -0.144792     -0.217132       -0.034039   \n",
      "6388              3     4     -0.144792     -0.217132       -2.616469   \n",
      "15117             0     4     -0.144792     -0.217132       -0.034039   \n",
      "5239              0     4      0.886847     -0.217132       -0.034039   \n",
      "\n",
      "       native_country  \n",
      "7061               38  \n",
      "9169               38  \n",
      "6388               38  \n",
      "15117              38  \n",
      "5239               38  \n"
     ]
    }
   ],
   "source": [
    "print(prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that categorical fields, like occupation, have already been converted to integers (with the same mapping that was used for training). Numerical fields, like age, have been scaled to a z-score. Some fields have been dropped from the original data.\n",
    "\n",
    "Export the prediction input to a newline-delimited JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('test.json', 'w') as json_file:\n",
    "  for row in prediction_input.values.tolist():\n",
    "    json.dump(row, json_file)\n",
    "    json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the .json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.901212751865387, 4.0, -0.030303768813610077, 2.0, 3.0, 0.0, 4.0, -0.1447920799255371, 4.688376426696777, 1.4185774326324463, 38.0]\n",
      "[1.6305595636367798, 3.0, -0.8082264065742493, 2.0, 2.0, 5.0, 2.0, -0.1447920799255371, -0.2171318680047989, -0.03403923660516739, 38.0]\n",
      "[-1.43269681930542, 6.0, -0.030303768813610077, 4.0, 0.0, 3.0, 4.0, -0.1447920799255371, -0.2171318680047989, -2.616468906402588, 38.0]\n",
      "[-0.2657420337200165, 3.0, -0.41926509141921997, 2.0, 0.0, 0.0, 4.0, -0.1447920799255371, -0.2171318680047989, -0.03403923660516739, 38.0]\n",
      "[1.5576248168945312, 3.0, 1.1365801095962524, 2.0, 9.0, 0.0, 4.0, 0.8868470191955566, -0.2171318680047989, -0.03403923660516739, 38.0]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4: Use your trained model for prediction\n",
    "\n",
    "Once you've trained your TensorFlow model, you can use it for prediction on new data. In this case, you've trained a census model to predict income category given some information about a person.\n",
    "\n",
    "Run the following command to run prediction on the test.json file we created above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get a \"Bad magic number in .pyc file\" error, go to the terminal and run:\n",
    "> cd ../../usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine/\n",
    "\n",
    "> sudo rm *.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2023-09-28 16:33:47.411540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-28 16:33:47.411672: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-28 16:33:47.411733: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (census-income-lab): /proc/driver/nvidia/version does not exist\n",
      "2023-09-28 16:33:47.412295: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:235: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:235: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:root:Error updating signature __saved_model_init_op: The name 'NoOp' refers to an Operation, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE_4\n",
      "[0.8129585981369019]\n",
      "[0.17566004395484924]\n",
      "[0.0002862215042114258]\n",
      "[0.2874601483345032]\n",
      "[0.9953505992889404]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform local predict \\\n",
    "    --model-dir output/keras_export/ \\\n",
    "    --json-instances ./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model's last layer uses a sigmoid function for its activation, outputs between 0 and 0.5 represent negative predictions **(\"<=50K\")** and outputs between 0.5 and 1 represent positive ones **(\">50K\")**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run your training job in the cloud\n",
    "\n",
    "Now that you've validated your model by running it locally, you will now get practice training using Cloud AI Platform.\n",
    "\n",
    "**Note:** The initial job request will take several minutes to start, but subsequent jobs run more quickly. This enables quick iteration as you develop and validate your training job.\n",
    "\n",
    "First, set the following variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-04-39d783c59a3c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-04-39d783c59a3c\"  # Replace with your project name\n",
    "BUCKET_NAME=PROJECT+\"-aiplatform\"\n",
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET_NAME\"] = BUCKET_NAME\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Set up a Cloud Storage bucket\n",
    "\n",
    "The AI Platform services need to access Cloud Storage (GCS) to read and write data during model training and batch prediction.\n",
    "\n",
    "Create a bucket using BUCKET_NAME as the name for the bucket and copy the data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/...\n",
      "Copying file://data/test.json [Content-Type=application/json]...\n",
      "Copying file://data/test.csv [Content-Type=text/csv]...                         \n",
      "Copying file://data/census.train.csv [Content-Type=text/csv]...                 \n",
      "Copying file://data/census.test.csv [Content-Type=text/csv]...                  \n",
      "- [4 files][  5.0 MiB/  5.0 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://data/adult.test.csv [Content-Type=text/csv]...\n",
      "Copying file://data/adult.data.csv [Content-Type=text/csv]...                   \n",
      "\\ [6 files][ 10.7 MiB/ 10.7 MiB]                                                \n",
      "Operation completed over 6 objects/10.7 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if ! gsutil ls | grep -q gs://${BUCKET_NAME}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "fi\n",
    "gsutil cp -r data gs://$BUCKET_NAME/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the TRAIN_DATA and EVAL_DATA variables to point to the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export TRAIN_DATA=gs://$BUCKET_NAME/data/adult.data.csv\n",
    "export EVAL_DATA=gs://$BUCKET_NAME/data/adult.test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gsutil again to copy the JSON test file test.json to your Cloud Storage bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://test.json [Content-Type=application/json]...\n",
      "/ [1 files][  681.0 B/  681.0 B]                                                \n",
      "Operation completed over 1 objects/681.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cp test.json gs://$BUCKET_NAME/data/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the TEST_JSON variable to point to that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export TEST_JSON=gs://$BUCKET_NAME/data/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go back to the lab instructions and check your progress by testing the completed tasks:**\n",
    "\n",
    "**- \"Set up a Google Cloud Storage\".**\n",
    "\n",
    "**- \"Upload the data files to your Cloud Storage bucket\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Run a single-instance trainer in the cloud\n",
    "\n",
    "With a validated training job that runs in both single-instance and distributed mode, you're now ready to run a training job in the cloud. For this example, we will be requesting a single-instance training job.\n",
    "\n",
    "Use the default BASIC scale tier to run a single-instance training job. The initial job request can take a few minutes to start, but subsequent jobs run more quickly. This enables quick iteration as you develop and validate your training job.\n",
    "\n",
    "Select a name for the initial training run that distinguishes it from any subsequent training runs. For example, we can use date and time to compose the job id.\n",
    "\n",
    "Specify a directory for output generated by AI Platform by setting an OUTPUT_PATH variable to include when requesting training and prediction jobs. The OUTPUT_PATH represents the fully qualified Cloud Storage location for model checkpoints, summaries, and exports. You can use the BUCKET_NAME variable you defined in a previous step. It's a good practice to use the job name as the output directory.\n",
    "\n",
    "Run the following command to submit a training job in the cloud that uses a single process. This time, set the --verbosity tag to DEBUG so that you can inspect the full logging output and retrieve accuracy, loss, and other metrics. The output also contains a number of other warning messages that you can ignore for the purposes of this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [census_230928_164417] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe census_230928_164417\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs census_230928_164417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: census_230928_164417\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "JOB_ID=census_$(date -u +%y%m%d_%H%M%S)\n",
    "OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_ID\n",
    "gcloud ai-platform jobs submit training $JOB_ID \\\n",
    "    --job-dir $OUTPUT_PATH \\\n",
    "    --runtime-version $TFVERSION \\\n",
    "    --python-version $PYTHONVERSION \\\n",
    "    --module-name trainer.task \\\n",
    "    --package-path trainer/ \\\n",
    "    --region $REGION \\\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 1000 \\\n",
    "    --eval-steps 100 \\\n",
    "    --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an environment variable with the jobId generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_ID\"] = \"census_230928_163751\" # Replace with your job id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the progress of your training job by watching the logs on the command line by running:\n",
    "\n",
    "`gcloud ai-platform jobs stream-logs $JOB_ID`\n",
    "\n",
    "Or monitor it in the Console at `AI Platform > Jobs`. Wait until your AI Platform training job is done. It is finished when you see a green check mark by the jobname in the Cloud Console, or when you see the message Job completed successfully from the Cloud Shell command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2023-09-28 16:37:54 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2023-09-28 16:38:09 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2023-09-28 16:38:09 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2023-09-28 16:38:09 +0000\tservice\t\tJob census_230928_163751 is queued.\n",
      "INFO\t2023-09-28 16:38:12 +0000\tservice\t\tWaiting for training program to start.\n",
      "INFO\t2023-09-28 16:39:04 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"chief\": [\"127.0.0.1:2222\"]} --task={\"type\": \"chief\", \"index\": 0} --job={  \"package_uris\": [\"gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/packages/f6887f70b585f4f2f19e59ae4da6254bbda7215f7bff58fd2c1e64697d390cd4/trainer-0.0.0.tar.gz\"],  \"python_module\": \"trainer.task\",  \"args\": [\"--train-files\", \"--eval-files\", \"--train-steps\", \"1000\", \"--eval-steps\", \"100\", \"--verbosity\", \"DEBUG\"],  \"region\": \"us-central1\",  \"runtime_version\": \"2.1\",  \"job_dir\": \"gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751\",  \"run_on_raw_vm\": true,  \"python_version\": \"3.7\"}\n",
      "WARNING\t2023-09-28 16:39:10 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "WARNING\t2023-09-28 16:39:10 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2023-09-28 16:39:10 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2023-09-28 16:39:14 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2023-09-28 16:39:14 +0000\tmaster-replica-0\t\tDownloading the package: gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/packages/f6887f70b585f4f2f19e59ae4da6254bbda7215f7bff58fd2c1e64697d390cd4/trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:14 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/packages/f6887f70b585f4f2f19e59ae4da6254bbda7215f7bff58fd2c1e64697d390cd4/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:15 +0000\tmaster-replica-0\t\tInstalling the package: gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/packages/f6887f70b585f4f2f19e59ae4da6254bbda7215f7bff58fd2c1e64697d390cd4/trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:15 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning egg_info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating /tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting /tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/PKG-INFO\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting dependency_links to /tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/dependency_links.txt\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting top-level names to /tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/top_level.txt\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting manifest file '/tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\treading manifest file '/tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting manifest file '/tmp/pip-pip-egg-info-12ll_1wl/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\t  Building wheel for trainer (setup.py): started\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning bdist_wheel\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning build\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning build_py\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/lib\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying trainer/model.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying trainer/task.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying trainer/util.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying trainer/__init__.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tinstalling to build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning install\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning install_lib\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/model.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/task.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/util.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/__init__.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning install_egg_info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning egg_info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating trainer.egg-info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting trainer.egg-info/PKG-INFO\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting dependency_links to trainer.egg-info/dependency_links.txt\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting top-level names to trainer.egg-info/top_level.txt\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\treading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\twriting manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tCopying trainer.egg-info to build/bdist.linux-x86_64/wheel/trainer-0.0.0-py3.7.egg-info\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\trunning install_scripts\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel/trainer-0.0.0.dist-info/WHEEL\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tcreating '/tmp/pip-wheel-xvoonop2/trainer-0.0.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer/model.py'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer/util.py'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tremoving build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\t  Building wheel for trainer (setup.py): finished with status 'done'\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\t  Created wheel for trainer: filename=trainer-0.0.0-py3-none-any.whl size=6836 sha256=9194cd2f6eedeae693362f149938c84179c57cfa29a596f5edf017d531fc80a3\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/23/ad/57/8e99a4eeb034edca80b0624cb205859d79e985819276530408\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2023-09-28 16:39:16 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "ERROR\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 23.2.1 is available.\n",
      "ERROR\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tRunning command: pip3 install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\trunning egg_info\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tcreating /tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\twriting /tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/PKG-INFO\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\twriting dependency_links to /tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/dependency_links.txt\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\twriting top-level names to /tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/top_level.txt\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\twriting manifest file '/tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\treading manifest file '/tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\twriting manifest file '/tmp/pip-pip-egg-info-4ncfc2s6/trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2023-09-28 16:39:17 +0000\tmaster-replica-0\t\t  Building wheel for trainer (setup.py): started\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning bdist_wheel\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning build\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning build_py\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/lib\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying trainer/model.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying trainer/task.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying trainer/util.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying trainer/__init__.py -> build/lib/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tinstalling to build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning install\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning install_lib\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/model.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/task.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/util.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcopying build/lib/trainer/__init__.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning install_egg_info\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning egg_info\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating trainer.egg-info\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\twriting trainer.egg-info/PKG-INFO\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\twriting dependency_links to trainer.egg-info/dependency_links.txt\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\twriting top-level names to trainer.egg-info/top_level.txt\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\twriting manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\treading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\twriting manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tCopying trainer.egg-info to build/bdist.linux-x86_64/wheel/trainer-0.0.0-py3.7.egg-info\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\trunning install_scripts\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating build/bdist.linux-x86_64/wheel/trainer-0.0.0.dist-info/WHEEL\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tcreating '/tmp/pip-wheel-oo8a5tel/trainer-0.0.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer/model.py'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer/util.py'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tremoving build/bdist.linux-x86_64/wheel\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t  Building wheel for trainer (setup.py): finished with status 'done'\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t  Created wheel for trainer: filename=trainer-0.0.0-py3-none-any.whl size=6836 sha256=fe452dcc47c55371eab826b212789fec5fc067215b365953958ea89444051f85\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/23/ad/57/8e99a4eeb034edca80b0624cb205859d79e985819276530408\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t  Attempting uninstall: trainer\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t    Found existing installation: trainer 0.0.0\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t    Uninstalling trainer-0.0.0:\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\t      Successfully uninstalled trainer-0.0.0\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "ERROR\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tWARNING: You are using pip version 20.1; however, version 23.2.1 is available.\n",
      "ERROR\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\n",
      "INFO\t2023-09-28 16:39:18 +0000\tmaster-replica-0\t\tRunning command: python3 -m trainer.task --train-files --eval-files --train-steps 1000 --eval-steps 100 --verbosity DEBUG --job-dir gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751\n",
      "WARNING\t2023-09-28 16:39:19 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "WARNING\t2023-09-28 16:39:19 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "WARNING\t2023-09-28 16:39:19 +0000\tmaster-replica-0\t\tCannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tInitializing local devices since in-graph multi-worker training with `MirroredStrategy` is not supported in eager mode. TF_CONFIG will be ignored when when initializing `MirroredStrategy`.\n",
      "WARNING\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tfailed call to cuInit: UNKNOWN ERROR (303)\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tkernel driver does not appear to be running on this host (cmle-training-193463183220979211): /proc/driver/nvidia/version does not exist\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tYour CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tCPU Frequency: 2299995000 Hz\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tXLA service 0x1eb6410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\t  StreamExecutor device (0): Host, Default Version\n",
      "WARNING\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tThere are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO\t2023-09-28 16:39:20 +0000\tmaster-replica-0\t\tUsing MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tProfiler session started.\n",
      "WARNING\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tCould not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tfunction cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "ERROR\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tfunction cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "INFO\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tTrain for 254 steps, validate for 1 steps\n",
      "INFO\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tEpoch 00001: LearningRateScheduler reducing learning rate to 0.02.\n",
      "INFO\t2023-09-28 16:39:26 +0000\tmaster-replica-0\t\tEpoch 1/20\n",
      "ERROR\t2023-09-28 16:39:27 +0000\tmaster-replica-0\t\tfunction cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "INFO\t2023-09-28 16:39:27 +0000\tmaster-replica-0\t\t GpuTracer has collected 0 callback api events and 0 activity events.\n",
      "WARNING\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\tMethod (on_train_batch_end) is slow compared to the batch update (0.857804). Check your callbacks.\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 11:55 - loss: 0.6605 - accuracy: 0.695\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t  2/254 [..............................] - ETA: 9:32 - loss: 9.6028 - accuracy: 0.742\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 46s - loss: 1.4130 - accuracy: 0.7164\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t 43/254 [====>.........................] - ETA: 22s - loss: 0.9785 - accuracy: 0.735\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t 66/254 [======>.......................] - ETA: 13s - loss: 0.8029 - accuracy: 0.750\n",
      "INFO\t2023-09-28 16:39:28 +0000\tmaster-replica-0\t\t 87/254 [=========>....................] - ETA: 9s - loss: 0.7209 - accuracy: 0.757\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t110/254 [===========>..................] - ETA: 6s - loss: 0.6768 - accuracy: 0.75\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t131/254 [==============>...............] - ETA: 4s - loss: 0.6385 - accuracy: 0.76\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t154/254 [=================>............] - ETA: 3s - loss: 0.6042 - accuracy: 0.77\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t175/254 [===================>..........] - ETA: 2s - loss: 0.5844 - accuracy: 0.77\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t198/254 [======================>.......] - ETA: 1s - loss: 0.5648 - accuracy: 0.78\n",
      "INFO\t2023-09-28 16:39:29 +0000\tmaster-replica-0\t\t218/254 [========================>.....] - ETA: 0s - loss: 0.5506 - accuracy: 0.78\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t241/254 [===========================>..] - ETA: 0s - loss: 0.5366 - accuracy: 0.78\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t254/254 [==============================] - 8s 30ms/step - loss: 0.5289 - accuracy: 0.7909 - val_loss: 0.4020 - val_accuracy: 0.8107\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\tEpoch 00002: LearningRateScheduler reducing learning rate to 0.015.\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\tEpoch 2/20\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.4983 - accuracy: 0.75\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.3681 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3719 - accuracy: 0.82\n",
      "INFO\t2023-09-28 16:39:31 +0000\tmaster-replica-0\t\t 66/254 [======>.......................] - ETA: 0s - loss: 0.3738 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t 88/254 [=========>....................] - ETA: 0s - loss: 0.3712 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t109/254 [===========>..................] - ETA: 0s - loss: 0.3695 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t132/254 [==============>...............] - ETA: 0s - loss: 0.3699 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t154/254 [=================>............] - ETA: 0s - loss: 0.3661 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t176/254 [===================>..........] - ETA: 0s - loss: 0.3637 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t196/254 [======================>.......] - ETA: 0s - loss: 0.3623 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t219/254 [========================>.....] - ETA: 0s - loss: 0.3614 - accuracy: 0.83\n",
      "WARNING\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t240/254 [===========================>..] - ETA: 0s - loss: 0.3629 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3626 - accuracy: 0.8343 - val_loss: 0.3284 - val_accuracy: 0.8466\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\tEpoch 00003: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\tEpoch 3/20\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3532 - accuracy: 0.82\n",
      "INFO\t2023-09-28 16:39:32 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3395 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t 44/254 [====>.........................] - ETA: 0s - loss: 0.3436 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3447 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t 88/254 [=========>....................] - ETA: 0s - loss: 0.3410 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t110/254 [===========>..................] - ETA: 0s - loss: 0.3418 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t131/254 [==============>...............] - ETA: 0s - loss: 0.3429 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t154/254 [=================>............] - ETA: 0s - loss: 0.3441 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t175/254 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t197/254 [======================>.......] - ETA: 0s - loss: 0.3436 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\t218/254 [========================>.....] - ETA: 0s - loss: 0.3416 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:33 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t241/254 [===========================>..] - ETA: 0s - loss: 0.3439 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3444 - accuracy: 0.8407 - val_loss: 0.3293 - val_accuracy: 0.8471\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\tEpoch 00004: LearningRateScheduler reducing learning rate to 0.01125.\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\tEpoch 4/20\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 1s - loss: 0.3507 - accuracy: 0.81\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3267 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3395 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t 69/254 [=======>......................] - ETA: 0s - loss: 0.3352 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t 88/254 [=========>....................] - ETA: 0s - loss: 0.3371 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t111/254 [============>.................] - ETA: 0s - loss: 0.3368 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t132/254 [==============>...............] - ETA: 0s - loss: 0.3379 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t155/254 [=================>............] - ETA: 0s - loss: 0.3357 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t176/254 [===================>..........] - ETA: 0s - loss: 0.3371 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t199/254 [======================>.......] - ETA: 0s - loss: 0.3369 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\t220/254 [========================>.....] - ETA: 0s - loss: 0.3355 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:34 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t241/254 [===========================>..] - ETA: 0s - loss: 0.3360 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3373 - accuracy: 0.8445 - val_loss: 0.3555 - val_accuracy: 0.8464\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\tEpoch 00005: LearningRateScheduler reducing learning rate to 0.010625.\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\tEpoch 5/20\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3488 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t 21/254 [=>............................] - ETA: 0s - loss: 0.3307 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t 43/254 [====>.........................] - ETA: 0s - loss: 0.3297 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t 64/254 [======>.......................] - ETA: 0s - loss: 0.3311 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t 86/254 [=========>....................] - ETA: 0s - loss: 0.3333 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t106/254 [===========>..................] - ETA: 0s - loss: 0.3340 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t129/254 [==============>...............] - ETA: 0s - loss: 0.3331 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t151/254 [================>.............] - ETA: 0s - loss: 0.3347 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t174/254 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t195/254 [======================>.......] - ETA: 0s - loss: 0.3352 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\t216/254 [========================>.....] - ETA: 0s - loss: 0.3350 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:35 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t238/254 [===========================>..] - ETA: 0s - loss: 0.3356 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8451 - val_loss: 0.3248 - val_accuracy: 0.8486\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\tEpoch 00006: LearningRateScheduler reducing learning rate to 0.0103125.\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\tEpoch 6/20\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3307 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3288 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t 44/254 [====>.........................] - ETA: 0s - loss: 0.3280 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3230 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t 89/254 [=========>....................] - ETA: 0s - loss: 0.3290 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t112/254 [============>.................] - ETA: 0s - loss: 0.3326 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t134/254 [==============>...............] - ETA: 0s - loss: 0.3322 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t157/254 [=================>............] - ETA: 0s - loss: 0.3359 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t179/254 [====================>.........] - ETA: 0s - loss: 0.3337 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t202/254 [======================>.......] - ETA: 0s - loss: 0.3347 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:36 +0000\tmaster-replica-0\t\t224/254 [=========================>....] - ETA: 0s - loss: 0.3345 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t244/254 [===========================>..] - ETA: 0s - loss: 0.3344 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3334 - accuracy: 0.8456 - val_loss: 0.3325 - val_accuracy: 0.8465\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\tEpoch 00007: LearningRateScheduler reducing learning rate to 0.01015625.\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\tEpoch 7/20\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3320 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3383 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3439 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3370 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t 90/254 [=========>....................] - ETA: 0s - loss: 0.3327 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t112/254 [============>.................] - ETA: 0s - loss: 0.3350 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t133/254 [==============>...............] - ETA: 0s - loss: 0.3349 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t156/254 [=================>............] - ETA: 0s - loss: 0.3334 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:37 +0000\tmaster-replica-0\t\t178/254 [====================>.........] - ETA: 0s - loss: 0.3325 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t199/254 [======================>.......] - ETA: 0s - loss: 0.3344 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t221/254 [=========================>....] - ETA: 0s - loss: 0.3339 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t243/254 [===========================>..] - ETA: 0s - loss: 0.3320 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8462 - val_loss: 0.3257 - val_accuracy: 0.8475\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\tEpoch 00008: LearningRateScheduler reducing learning rate to 0.010078125.\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\tEpoch 8/20\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3734 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3241 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t 46/254 [====>.........................] - ETA: 0s - loss: 0.3107 - accuracy: 0.86\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3208 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t 91/254 [=========>....................] - ETA: 0s - loss: 0.3248 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:38 +0000\tmaster-replica-0\t\t113/254 [============>.................] - ETA: 0s - loss: 0.3256 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t135/254 [==============>...............] - ETA: 0s - loss: 0.3268 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t157/254 [=================>............] - ETA: 0s - loss: 0.3268 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t179/254 [====================>.........] - ETA: 0s - loss: 0.3271 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t201/254 [======================>.......] - ETA: 0s - loss: 0.3286 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t224/254 [=========================>....] - ETA: 0s - loss: 0.3291 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t246/254 [============================>.] - ETA: 0s - loss: 0.3299 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3304 - accuracy: 0.8485 - val_loss: 0.3259 - val_accuracy: 0.8466\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\tEpoch 00009: LearningRateScheduler reducing learning rate to 0.0100390625.\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\tEpoch 9/20\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 1s - loss: 0.2982 - accuracy: 0.81\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.3341 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:39 +0000\tmaster-replica-0\t\t 44/254 [====>.........................] - ETA: 0s - loss: 0.3287 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3269 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t 89/254 [=========>....................] - ETA: 0s - loss: 0.3235 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t111/254 [============>.................] - ETA: 0s - loss: 0.3259 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t132/254 [==============>...............] - ETA: 0s - loss: 0.3271 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t155/254 [=================>............] - ETA: 0s - loss: 0.3270 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t177/254 [===================>..........] - ETA: 0s - loss: 0.3276 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t199/254 [======================>.......] - ETA: 0s - loss: 0.3281 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t221/254 [=========================>....] - ETA: 0s - loss: 0.3274 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t243/254 [===========================>..] - ETA: 0s - loss: 0.3295 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8471 - val_loss: 0.3480 - val_accuracy: 0.8475\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\tEpoch 00010: LearningRateScheduler reducing learning rate to 0.01001953125.\n",
      "INFO\t2023-09-28 16:39:40 +0000\tmaster-replica-0\t\tEpoch 10/20\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 1s - loss: 0.4025 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3419 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t 46/254 [====>.........................] - ETA: 0s - loss: 0.3291 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3348 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t 91/254 [=========>....................] - ETA: 0s - loss: 0.3336 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t114/254 [============>.................] - ETA: 0s - loss: 0.3316 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t137/254 [===============>..............] - ETA: 0s - loss: 0.3275 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t158/254 [=================>............] - ETA: 0s - loss: 0.3284 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t180/254 [====================>.........] - ETA: 0s - loss: 0.3278 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t201/254 [======================>.......] - ETA: 0s - loss: 0.3295 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\t224/254 [=========================>....] - ETA: 0s - loss: 0.3304 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:41 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t246/254 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8473 - val_loss: 0.3518 - val_accuracy: 0.8494\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\tEpoch 00011: LearningRateScheduler reducing learning rate to 0.010009765625.\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\tEpoch 11/20\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3477 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3195 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3197 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3213 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t 90/254 [=========>....................] - ETA: 0s - loss: 0.3265 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t113/254 [============>.................] - ETA: 0s - loss: 0.3269 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t135/254 [==============>...............] - ETA: 0s - loss: 0.3294 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t158/254 [=================>............] - ETA: 0s - loss: 0.3294 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t180/254 [====================>.........] - ETA: 0s - loss: 0.3293 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t203/254 [======================>.......] - ETA: 0s - loss: 0.3285 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\t225/254 [=========================>....] - ETA: 0s - loss: 0.3301 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:42 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t248/254 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.8481 - val_loss: 0.3285 - val_accuracy: 0.8472\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\tEpoch 00012: LearningRateScheduler reducing learning rate to 0.010004882812500001.\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\tEpoch 12/20\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 1s - loss: 0.2773 - accuracy: 0.87\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t 21/254 [=>............................] - ETA: 0s - loss: 0.3123 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t 42/254 [===>..........................] - ETA: 0s - loss: 0.3063 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t 64/254 [======>.......................] - ETA: 0s - loss: 0.3085 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t 86/254 [=========>....................] - ETA: 0s - loss: 0.3167 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t108/254 [===========>..................] - ETA: 0s - loss: 0.3213 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t130/254 [==============>...............] - ETA: 0s - loss: 0.3239 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t152/254 [================>.............] - ETA: 0s - loss: 0.3224 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t175/254 [===================>..........] - ETA: 0s - loss: 0.3243 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t198/254 [======================>.......] - ETA: 0s - loss: 0.3240 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\t221/254 [=========================>....] - ETA: 0s - loss: 0.3239 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:43 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t244/254 [===========================>..] - ETA: 0s - loss: 0.3248 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8491 - val_loss: 0.3266 - val_accuracy: 0.8502\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\tEpoch 00013: LearningRateScheduler reducing learning rate to 0.01000244140625.\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\tEpoch 13/20\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.4047 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t 24/254 [=>............................] - ETA: 0s - loss: 0.3420 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t 46/254 [====>.........................] - ETA: 0s - loss: 0.3355 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t 69/254 [=======>......................] - ETA: 0s - loss: 0.3286 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t 92/254 [=========>....................] - ETA: 0s - loss: 0.3239 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t114/254 [============>.................] - ETA: 0s - loss: 0.3241 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t137/254 [===============>..............] - ETA: 0s - loss: 0.3274 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t160/254 [=================>............] - ETA: 0s - loss: 0.3284 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t181/254 [====================>.........] - ETA: 0s - loss: 0.3280 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t204/254 [=======================>......] - ETA: 0s - loss: 0.3282 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\t227/254 [=========================>....] - ETA: 0s - loss: 0.3278 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:44 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t248/254 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3269 - accuracy: 0.8492 - val_loss: 0.3397 - val_accuracy: 0.8446\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\tEpoch 00014: LearningRateScheduler reducing learning rate to 0.010001220703125.\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\tEpoch 14/20\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.2708 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.2927 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t 44/254 [====>.........................] - ETA: 0s - loss: 0.3085 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3123 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t 90/254 [=========>....................] - ETA: 0s - loss: 0.3166 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t113/254 [============>.................] - ETA: 0s - loss: 0.3183 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t136/254 [===============>..............] - ETA: 0s - loss: 0.3190 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t158/254 [=================>............] - ETA: 0s - loss: 0.3225 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:45 +0000\tmaster-replica-0\t\t180/254 [====================>.........] - ETA: 0s - loss: 0.3208 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t202/254 [======================>.......] - ETA: 0s - loss: 0.3227 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t225/254 [=========================>....] - ETA: 0s - loss: 0.3224 - accuracy: 0.85\n",
      "WARNING\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t247/254 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8509 - val_loss: 0.3249 - val_accuracy: 0.8478\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\tEpoch 00015: LearningRateScheduler reducing learning rate to 0.0100006103515625.\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\tEpoch 15/20\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3090 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.3372 - accuracy: 0.83\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3289 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3304 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t 89/254 [=========>....................] - ETA: 0s - loss: 0.3300 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:46 +0000\tmaster-replica-0\t\t111/254 [============>.................] - ETA: 0s - loss: 0.3270 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t134/254 [==============>...............] - ETA: 0s - loss: 0.3270 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t156/254 [=================>............] - ETA: 0s - loss: 0.3290 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t178/254 [====================>.........] - ETA: 0s - loss: 0.3301 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t200/254 [======================>.......] - ETA: 0s - loss: 0.3291 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t222/254 [=========================>....] - ETA: 0s - loss: 0.3281 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t245/254 [===========================>..] - ETA: 0s - loss: 0.3257 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8497 - val_loss: 0.3264 - val_accuracy: 0.8494\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\tEpoch 00016: LearningRateScheduler reducing learning rate to 0.01000030517578125.\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\tEpoch 16/20\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 1s - loss: 0.3292 - accuracy: 0.82\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.3160 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:47 +0000\tmaster-replica-0\t\t 42/254 [===>..........................] - ETA: 0s - loss: 0.3196 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t 63/254 [======>.......................] - ETA: 0s - loss: 0.3201 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t 84/254 [========>.....................] - ETA: 0s - loss: 0.3222 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t106/254 [===========>..................] - ETA: 0s - loss: 0.3218 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t127/254 [==============>...............] - ETA: 0s - loss: 0.3243 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t149/254 [================>.............] - ETA: 0s - loss: 0.3246 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t171/254 [===================>..........] - ETA: 0s - loss: 0.3258 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t193/254 [=====================>........] - ETA: 0s - loss: 0.3237 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t216/254 [========================>.....] - ETA: 0s - loss: 0.3250 - accuracy: 0.85\n",
      "WARNING\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t239/254 [===========================>..] - ETA: 0s - loss: 0.3245 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3244 - accuracy: 0.8513 - val_loss: 0.3311 - val_accuracy: 0.8524\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\tEpoch 00017: LearningRateScheduler reducing learning rate to 0.010000152587890625.\n",
      "INFO\t2023-09-28 16:39:48 +0000\tmaster-replica-0\t\tEpoch 17/20\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.4333 - accuracy: 0.78\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3371 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t 46/254 [====>.........................] - ETA: 0s - loss: 0.3288 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3300 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t 88/254 [=========>....................] - ETA: 0s - loss: 0.3262 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t111/254 [============>.................] - ETA: 0s - loss: 0.3228 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t130/254 [==============>...............] - ETA: 0s - loss: 0.3266 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t151/254 [================>.............] - ETA: 0s - loss: 0.3247 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t173/254 [===================>..........] - ETA: 0s - loss: 0.3247 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t195/254 [======================>.......] - ETA: 0s - loss: 0.3244 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\t216/254 [========================>.....] - ETA: 0s - loss: 0.3243 - accuracy: 0.85\n",
      "WARNING\t2023-09-28 16:39:49 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t238/254 [===========================>..] - ETA: 0s - loss: 0.3256 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8492 - val_loss: 0.3294 - val_accuracy: 0.8508\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\tEpoch 00018: LearningRateScheduler reducing learning rate to 0.010000076293945313.\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\tEpoch 18/20\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3237 - accuracy: 0.89\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3096 - accuracy: 0.86\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3165 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3206 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t 91/254 [=========>....................] - ETA: 0s - loss: 0.3220 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t114/254 [============>.................] - ETA: 0s - loss: 0.3214 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t136/254 [===============>..............] - ETA: 0s - loss: 0.3241 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t158/254 [=================>............] - ETA: 0s - loss: 0.3242 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t181/254 [====================>.........] - ETA: 0s - loss: 0.3245 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t204/254 [=======================>......] - ETA: 0s - loss: 0.3250 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\t227/254 [=========================>....] - ETA: 0s - loss: 0.3244 - accuracy: 0.85\n",
      "WARNING\t2023-09-28 16:39:50 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t250/254 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3248 - accuracy: 0.8501 - val_loss: 0.3297 - val_accuracy: 0.8511\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\tEpoch 00019: LearningRateScheduler reducing learning rate to 0.010000038146972657.\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\tEpoch 19/20\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3171 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t 22/254 [=>............................] - ETA: 0s - loss: 0.3236 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t 45/254 [====>.........................] - ETA: 0s - loss: 0.3164 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t 68/254 [=======>......................] - ETA: 0s - loss: 0.3186 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t 91/254 [=========>....................] - ETA: 0s - loss: 0.3186 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t113/254 [============>.................] - ETA: 0s - loss: 0.3211 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t135/254 [==============>...............] - ETA: 0s - loss: 0.3210 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t158/254 [=================>............] - ETA: 0s - loss: 0.3204 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t181/254 [====================>.........] - ETA: 0s - loss: 0.3234 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t203/254 [======================>.......] - ETA: 0s - loss: 0.3239 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\t226/254 [=========================>....] - ETA: 0s - loss: 0.3241 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:51 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t249/254 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.84\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 4ms/step - loss: 0.3243 - accuracy: 0.8495 - val_loss: 0.3274 - val_accuracy: 0.8473\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\tEpoch 00020: LearningRateScheduler reducing learning rate to 0.010000019073486329.\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\tEpoch 20/20\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t  1/254 [..............................] - ETA: 0s - loss: 0.3699 - accuracy: 0.81\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t 23/254 [=>............................] - ETA: 0s - loss: 0.3230 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t 46/254 [====>.........................] - ETA: 0s - loss: 0.3244 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t 67/254 [======>.......................] - ETA: 0s - loss: 0.3248 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t 90/254 [=========>....................] - ETA: 0s - loss: 0.3233 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t112/254 [============>.................] - ETA: 0s - loss: 0.3224 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t134/254 [==============>...............] - ETA: 0s - loss: 0.3235 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t157/254 [=================>............] - ETA: 0s - loss: 0.3223 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t180/254 [====================>.........] - ETA: 0s - loss: 0.3220 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t202/254 [======================>.......] - ETA: 0s - loss: 0.3236 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\t225/254 [=========================>....] - ETA: 0s - loss: 0.3251 - accuracy: 0.84\n",
      "WARNING\t2023-09-28 16:39:52 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "INFO\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\t247/254 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.85\n",
      "INFO\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\t254/254 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8506 - val_loss: 0.3308 - val_accuracy: 0.8521\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tError occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tSets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2023-09-28 16:39:53 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2023-09-28 16:39:57 +0000\tmaster-replica-0\t\tAssets written to: gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/keras_export/assets\n",
      "INFO\t2023-09-28 16:39:57 +0000\tmaster-replica-0\t\tModel exported to: gs://qwiklabs-gcp-04-39d783c59a3c-aiplatform/census_230928_163751/keras_export\n",
      "INFO\t2023-09-28 16:39:58 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2023-09-28 16:39:58 +0000\tmaster-replica-0\t\tClean up finished.\n",
      "INFO\t2023-09-28 16:39:58 +0000\tmaster-replica-0\t\tTask completed successfully.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform jobs stream-logs $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait for the job to complete before proceeding to the next step.\n",
    "Go back to the lab instructions and check your progress by testing the completed task:**\n",
    "\n",
    "**- \"Run a single-instance trainer in the cloud\".**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Deploy your model to support prediction\n",
    "\n",
    "By deploying your trained model to AI Platform to serve online prediction requests, you get the benefit of scalable serving. This is useful if you expect your trained model to be hit with many prediction requests in a short period of time.\n",
    "\n",
    "**Note:** You will get `Using endpoint [https://ml.googleapis.com/]` output after running the next cells. If you try to open that link, you will see `404` error message. **You have to ignore it and move forward.**\n",
    "\n",
    "Create an AI Platform model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_NAME\"] = \"census\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ai platform model [projects/qwiklabs-gcp-04-39d783c59a3c/models/census].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform models create $MODEL_NAME --regions=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the environment variable MODEL_BINARIES to the full path of your exported trained model binaries `$OUTPUT_PATH/keras_export/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll deploy this trained model.\n",
    "\n",
    "Run the following command to create a version v1 of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......\n",
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_ID\n",
    "MODEL_BINARIES=$OUTPUT_PATH/keras_export/\n",
    "gcloud ai-platform versions create v1 \\\n",
    "--model $MODEL_NAME \\\n",
    "--origin $MODEL_BINARIES \\\n",
    "--runtime-version $TFVERSION \\\n",
    "--python-version $PYTHONVERSION \\\n",
    "--region=global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take several minutes to deploy your trained model. When done, you can see a list of your models using the models list command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    DEFAULT_VERSION_NAME\n",
      "census  v1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform models list --region=global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go back to the lab instructions and check your progress by testing the completed tasks:**\n",
    "\n",
    "**- \"Create an AI Platform model\".**\n",
    "\n",
    "**- \"Create a version v1 of your model\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Send an online prediction request to your deployed model\n",
    "\n",
    "You can now send prediction requests to your deployed model. The following command sends a prediction request using the test.json.\n",
    "\n",
    "The response includes the probabilities of each label **(>50K and <=50K)** based on the data entry in test.json, thus indicating whether the predicted income is greater than or less than 50,000 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE_4\n",
      "[0.6115581393241882]\n",
      "[0.3212571442127228]\n",
      "[0.0002480252005625516]\n",
      "[0.1394324153661728]\n",
      "[0.9508118629455566]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform predict \\\n",
    "--model $MODEL_NAME \\\n",
    "--version v1 \\\n",
    "--json-instances ./test.json \\\n",
    "--region global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** AI Platform supports batch prediction, too, but it's not included in this lab. See the documentation for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go back to the lab instructions to answer some multiple choice questions to reinforce your uncerstanding of some of these lab's concepts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "In this lab you've learned how to train a TensorFlow model both locally and on AI Platform, how to prepare data for prediction and to perform predictions both locally and in the Cloud AI Platform."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
